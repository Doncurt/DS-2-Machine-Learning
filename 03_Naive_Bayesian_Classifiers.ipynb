{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/ms_logo.jpeg' height=40% width=40%></center>\n",
    "\n",
    "<center><h1>Naive Bayesian Classifiers</h1></center>\n",
    "\n",
    "In this notebook, we'll be focusing on **_Naive Bayesian Classifiers_**, or \"Naive Bayes\" for short.  This is an algorithm that uses **_Bayes' Theorem_** to make a classification based on probability.  In case you're unfamiliar with Bayes' Theorem, let's look at the formula:\n",
    "<br>\n",
    "<br>\n",
    "<center><img src='img/bayes_theorem.png' height=40% width=40%></center>\n",
    "<br>\n",
    "<br>\n",
    "Don't worry if you've seen this mathematical notation before. In plain English, that formula reads:\n",
    "\n",
    "\"The probability of A given B equals the probability of B given A, times the probability of A, divided by the probability of B\".  \n",
    "\n",
    "Let's run through an example case here and see if we can demystify this equation a little bit more. \n",
    "\n",
    "<center><h3>Scenario: Spam Detection</h3></center>\n",
    "\n",
    "We have a dataset of emails, and we're trying to build a classifier that can predict if an email is spam or not by examining the words based in the emails.  Each email in our training set has been labeled as \"spam\" or \"ham\" (a real email, not spam).  We've counted each word used in every email, and found the following:\n",
    "\n",
    "**_65% of the emails in the dataset are \"Spam\"._**\n",
    "\n",
    "**_\"Spam\"_** emails contain the word _\"deal\"_ 80% of the time, and _\"win\"_ 40% of the time.  \n",
    "\n",
    "**_35% of the emails in the datasert are \"Ham\"._**\n",
    "\n",
    "**_\"Ham\"_** emails contain the word _\"deal\"_ 17% of the time, and _\"win\"_ 6% of the time.  \n",
    "\n",
    "The next email we try to predict contains the both words \"deal\" and \"win\". Given the information above, we can plug these numbers into Bayes' Theorem and predict the likelihood that this is email Spam. \n",
    "\n",
    "<center>P(Spam|deal, win) = (P(win, deal|Spam) * P(Spam)) / P(deal, win)</center>\n",
    "\n",
    "This can be further broken down into: \n",
    "<br>\n",
    "<br>\n",
    "<center>P(Spam|deal) \\* P(Spam|win) = P(deal|Spam) \\* P(Spam) \\*  P(win|Spam) \\* P(Spam) / P(deal|Spam) + P(deal|!Spam) \\* P(win|Spam) + P(win|!Spam)</center>\n",
    "\n",
    "In the equation above, \"P(deal|!Spam)\" can be read as \"the percentage that 'deal' occurs in 'Ham' emails\".  \n",
    "\n",
    "On the next step, we'll start defining the probabilities for everything in that equation so we can plug them in:\n",
    "\n",
    "1. P(deal|Spam) = .8\n",
    "1. P(win|Spam) = .4\n",
    "1. P(Spam) = .65\n",
    "1. P(deal|!Spam) = .17\n",
    "1. P(win|!Spam) = .06\n",
    "1. P(!Spam) = .35\n",
    "\n",
    "Let's replace some of these terms with the probabilities listed above and see how it works out:\n",
    "<br>\n",
    "<br>\n",
    "<center>(.8 \\* .65 \\* .4 \\* .65) / .8 \\* .65 + .35 \\* .17 \\* .4 \\* .65 + .35 \\* .6 = **0.922595** </center>\n",
    "<br>\n",
    "<br>\n",
    "Based on the math from Bayes' Theorem, we can predict probability that a new email containing both \"deal\" and \"win\" is \"Spam\" is approximately **92.2%**!\n",
    "\n",
    "<center><h3>Using Naive Bayes in the Real World</h3></center>\n",
    "\n",
    "In the above example, we did the math by hand.  That isn't very practical in the real world.  Luckily, `sklearn` contains some awesome implementations of Naive Bayesian Classifiers (and regressors!).  \n",
    "\n",
    "For this assignment, we're going to use a `GaussianNB()` object.  There are a few different kinds of Naive Bayesian Classifiers, but for this one we'll stick to one that assumes our data follows a Gaussian (normal) distribution.  \n",
    "\n",
    "Let's Get Started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 1 1 1 2 0 2 0 0 1 2 2 1 2 1 2 1 1 2 1 1 2 1 2 1 0 2 1 1 1 1 2 0 0 2 1 0 0\n",
      " 2]\n",
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 1 1 1 2 0 2 0 0 1 2 2 2 2 1 2 1 1 2 2 2 2 1 2 1 0 2 1 1 1 1 2 0 0 2 1 0 0\n",
      " 1]\n",
      "accuracy score: 0.9466666666666667\n",
      "f1 score: [ 1.          0.93548387  0.91304348]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "# Use load_iris() to load the data into the iris variable, and then assign iris.data and iris.target to the appropriate\n",
    "# variables\n",
    "iris = load_iris()\n",
    "data = iris.data\n",
    "labels = iris.target\n",
    "\n",
    "# Use train_test_split to split the data into X_train, X_test, y_train, and y_test variables\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.5)\n",
    "\n",
    "# Create a GaussianNB() object and fit it using the training data\n",
    "clf = GaussianNB()\n",
    "clf.fit(data,labels)\n",
    "# Use the fitted model to create predictions for the X_test data.\n",
    "preds =clf.predict(X_test)\n",
    "\n",
    "\n",
    "# Run it all and see how you did!\n",
    "print(preds)\n",
    "print(y_test)\n",
    "print(\"accuracy score: {}\".format(accuracy_score(y_test, preds)))\n",
    "print(\"f1 score: {}\".format(f1_score(y_test, preds, average=None)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Some Caveats</h3></center>\n",
    "\n",
    "You may have wondered why this particular model is a called a **_Naive_** Bayesian Classifier.  In this scenario, the word \"Naive\" simply means that the model makes the \"naive\" assumption that all features are independent of one another.  This leads us to the main caveat of this model--if you have feature columns that are highly correlated, this model may not work as well as we'd like.  **_If you're going to use Naive Bayes, make sure you check for highly correlated features beforehand!_**\n",
    "\n",
    "\n",
    "<center><h3>Where to Go From Here</h3></center>\n",
    "\n",
    "For the latter part of this assignment, you're going to use the famous Pima Indians Diabetes Dataset to build a Naive Bayesian Classifier that predicts whether or not an individual has diabetes.  You'll find the `pima_indians_diabetes.csv` file inside the `datasets` folder.  \n",
    "\n",
    "To build this classifier successfully, you'll want to follow the best practices for loading in and preprocessing a data set that you've learned in class:\n",
    "\n",
    "1. Importing the data\n",
    "1. Exploring the data\n",
    "1. \"Cleaning\" the data\n",
    "1. Splitting the data into training and testing sets (or using KFold Cross val--more on this below)\n",
    "1. Fitting the model\n",
    "1. Validating the model (checking predictions on the test set)\n",
    "\n",
    "Be sure to consider the following questions as you solve this problem:\n",
    "\n",
    "* How will you deal with null values?\n",
    "* For this model, does scaling the data improve your results? (HINT: test your assumption!)\n",
    "\n",
    "On top of cleaning and preprocessing this data set, you'll also use **_Cross Validation_** to get a better measure of the accuracy of your model.  We did not use K Fold Cross Validation in the above model on purpose--instead, you'll need to work your way through `sklearn`'s [model_selection documentation](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) to figure out how to effectively make use of cross-validation.  \n",
    "\n",
    "(**_Hint:_** There are several ways to implement cross validation using sklearn.  In the `model_selection` section of the documentation, pay special attention to the `KFold` object, as well as the methods available under the _Model Selection_ subsection.)\n",
    "\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0\n",
      " 0 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
      " 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
      " 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1\n",
      " 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0\n",
      " 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0\n",
      " 0 1 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0\n",
      " 0 0 0 1 0 0 0 1 1 0 1 0 0 1]\n",
      "158    0\n",
      "645    0\n",
      "42     0\n",
      "550    0\n",
      "589    0\n",
      "360    1\n",
      "432    0\n",
      "712    1\n",
      "472    0\n",
      "632    0\n",
      "15     1\n",
      "134    0\n",
      "687    0\n",
      "597    0\n",
      "421    0\n",
      "101    0\n",
      "197    1\n",
      "129    1\n",
      "523    1\n",
      "225    0\n",
      "456    0\n",
      "742    0\n",
      "602    0\n",
      "195    1\n",
      "39     1\n",
      "409    1\n",
      "16     1\n",
      "460    0\n",
      "604    1\n",
      "397    1\n",
      "      ..\n",
      "555    0\n",
      "577    1\n",
      "442    0\n",
      "627    0\n",
      "315    0\n",
      "745    0\n",
      "136    0\n",
      "583    0\n",
      "662    1\n",
      "282    0\n",
      "489    0\n",
      "133    0\n",
      "349    1\n",
      "375    1\n",
      "661    1\n",
      "1      0\n",
      "578    0\n",
      "70     1\n",
      "137    0\n",
      "43     1\n",
      "403    0\n",
      "400    1\n",
      "450    0\n",
      "53     1\n",
      "227    1\n",
      "735    0\n",
      "485    1\n",
      "167    0\n",
      "376    0\n",
      "595    1\n",
      "Name: Outcome, Length: 384, dtype: int64\n",
      "accuracy score: 0.7734375\n",
      "f1 score: [ 0.8256513   0.67657993]\n"
     ]
    }
   ],
   "source": [
    "# path to file: \"datasets/pima_indians_diabetes.csv\". The first row of the .csv contains the column names.\n",
    "# Note that in the \"Outcome\" column, 0 denotes someone that does NOT have diabetes, and 1 denotes someone that does.  \n",
    "import pandas as pd\n",
    "\n",
    "df= pd.read_csv(\"datasets/pima_indians_diabetes.csv\")\n",
    "#df\n",
    "target2 =df['Outcome']\n",
    "data1 =df.drop('Outcome', axis=1)\n",
    "#investigating for null values, doesnt seem to be any\n",
    "# for index, row in df.iterrows():\n",
    "#     print(row[3])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data1, target2, test_size=0.5)\n",
    "\n",
    "\n",
    "clf2 = GaussianNB()\n",
    "clf2.fit(data1,target)\n",
    "\n",
    "prediction =clf2.predict(X_test)\n",
    "print(prediction)\n",
    "print(y_test)\n",
    "print(\"accuracy score: {}\".format(accuracy_score(y_test, prediction)))\n",
    "print(\"f1 score: {}\".format(f1_score(y_test, prediction, average=None)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.79220779  0.70779221  0.73376623  0.79084967  0.71895425]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nSCORES ARE LOW DUE TO ME NOT DROPPING 0'S, SINCE NO-ONE CAN HAVE A SKIN THICKNESS OF 0 AND A A BMI OF ZERO, WILL FIX\\n\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "#define a kfold which takes in the lenth of the training set, shffles and has a set number of folds in this case 5\n",
    "k_fold = KFold(len(target2), n_folds=5, shuffle=True, random_state=0)\n",
    "#declare the classifier type as gaussian\n",
    "clf= GaussianNB()\n",
    "#print the folds as percentages, should print 5\n",
    "print( cross_val_score(clf, data1, target2, cv=k_fold, n_jobs=1) )\n",
    "\n",
    "'''\n",
    "SCORES ARE LOW DUE TO ME NOT DROPPING 0'S, SINCE NO-ONE CAN HAVE A SKIN THICKNESS OF 0 AND A A BMI OF ZERO, WILL FIX\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
